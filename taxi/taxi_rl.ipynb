{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gymnasium.farama.org/environments/toy_text/taxi/\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.9\n",
    "gamma = 0.95\n",
    "epsilon = 0.995\n",
    "epsilon_decay = 0.9995\n",
    "min_epsilon = 0.01 \n",
    "num_episodes = 10000\n",
    "max_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(q_table[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25 finished after 70 timesteps with reward -239\n",
      "Episode 29 finished after 40 timesteps with reward -128\n",
      "Episode 61 finished after 95 timesteps with reward -309\n",
      "Episode 179 finished after 95 timesteps with reward -354\n",
      "Episode 396 finished after 26 timesteps with reward -60\n",
      "Episode 429 finished after 57 timesteps with reward -190\n",
      "Episode 434 finished after 64 timesteps with reward -188\n",
      "Episode 439 finished after 42 timesteps with reward -139\n",
      "Episode 523 finished after 94 timesteps with reward -299\n",
      "Episode 557 finished after 95 timesteps with reward -408\n",
      "Episode 649 finished after 85 timesteps with reward -245\n",
      "Episode 670 finished after 98 timesteps with reward -339\n",
      "Episode 679 finished after 86 timesteps with reward -354\n",
      "Episode 807 finished after 78 timesteps with reward -292\n",
      "Episode 838 finished after 94 timesteps with reward -290\n",
      "Episode 862 finished after 79 timesteps with reward -248\n",
      "Episode 978 finished after 89 timesteps with reward -249\n",
      "Episode 1019 finished after 19 timesteps with reward -35\n",
      "Episode 1064 finished after 55 timesteps with reward -197\n",
      "Episode 1500 finished after 91 timesteps with reward -233\n",
      "Episode 1545 finished after 96 timesteps with reward -346\n",
      "Episode 1602 finished after 88 timesteps with reward -302\n",
      "Episode 1716 finished after 77 timesteps with reward -237\n",
      "Episode 1740 finished after 45 timesteps with reward -133\n",
      "Episode 1830 finished after 88 timesteps with reward -311\n",
      "Episode 1861 finished after 69 timesteps with reward -238\n",
      "Episode 2118 finished after 38 timesteps with reward -135\n",
      "Episode 2327 finished after 66 timesteps with reward -217\n",
      "Episode 2444 finished after 92 timesteps with reward -333\n",
      "Episode 2449 finished after 81 timesteps with reward -322\n",
      "Episode 2465 finished after 79 timesteps with reward -320\n",
      "Episode 2469 finished after 87 timesteps with reward -202\n",
      "Episode 2482 finished after 76 timesteps with reward -227\n",
      "Episode 2570 finished after 46 timesteps with reward -161\n",
      "Episode 2593 finished after 67 timesteps with reward -236\n",
      "Episode 2680 finished after 88 timesteps with reward -338\n",
      "Episode 2871 finished after 62 timesteps with reward -249\n",
      "Episode 2902 finished after 75 timesteps with reward -190\n",
      "Episode 2908 finished after 69 timesteps with reward -256\n",
      "Episode 2976 finished after 72 timesteps with reward -214\n",
      "Episode 3016 finished after 75 timesteps with reward -271\n",
      "Episode 3024 finished after 57 timesteps with reward -172\n",
      "Episode 3034 finished after 35 timesteps with reward -51\n",
      "Episode 3036 finished after 47 timesteps with reward -153\n",
      "Episode 3108 finished after 74 timesteps with reward -297\n",
      "Episode 3261 finished after 41 timesteps with reward -147\n",
      "Episode 3407 finished after 49 timesteps with reward -137\n",
      "Episode 3469 finished after 66 timesteps with reward -235\n",
      "Episode 3479 finished after 76 timesteps with reward -281\n",
      "Episode 3505 finished after 31 timesteps with reward -146\n",
      "Episode 3649 finished after 62 timesteps with reward -204\n",
      "Episode 3772 finished after 32 timesteps with reward -39\n",
      "Episode 3790 finished after 76 timesteps with reward -281\n",
      "Episode 3866 finished after 22 timesteps with reward -56\n",
      "Episode 3884 finished after 69 timesteps with reward -247\n",
      "Episode 3916 finished after 82 timesteps with reward -305\n",
      "Episode 3926 finished after 55 timesteps with reward -170\n",
      "Episode 4181 finished after 89 timesteps with reward -303\n",
      "Episode 4194 finished after 79 timesteps with reward -257\n",
      "Episode 4362 finished after 80 timesteps with reward -267\n",
      "Episode 4368 finished after 81 timesteps with reward -259\n",
      "Episode 4531 finished after 96 timesteps with reward -328\n",
      "Episode 4569 finished after 84 timesteps with reward -271\n",
      "Episode 4710 finished after 99 timesteps with reward -367\n",
      "Episode 4720 finished after 75 timesteps with reward -208\n",
      "Episode 4827 finished after 44 timesteps with reward -114\n",
      "Episode 4836 finished after 23 timesteps with reward -39\n",
      "Episode 4869 finished after 95 timesteps with reward -336\n",
      "Episode 4883 finished after 32 timesteps with reward -75\n",
      "Episode 4890 finished after 38 timesteps with reward -126\n",
      "Episode 5105 finished after 79 timesteps with reward -284\n",
      "Episode 5154 finished after 62 timesteps with reward -186\n",
      "Episode 5434 finished after 59 timesteps with reward -183\n",
      "Episode 5513 finished after 91 timesteps with reward -260\n",
      "Episode 5578 finished after 65 timesteps with reward -216\n",
      "Episode 5595 finished after 86 timesteps with reward -318\n",
      "Episode 5638 finished after 53 timesteps with reward -150\n",
      "Episode 5652 finished after 41 timesteps with reward -111\n",
      "Episode 5662 finished after 57 timesteps with reward -199\n",
      "Episode 5691 finished after 51 timesteps with reward -193\n",
      "Episode 5775 finished after 65 timesteps with reward -225\n",
      "Episode 5821 finished after 98 timesteps with reward -348\n",
      "Episode 5827 finished after 78 timesteps with reward -247\n",
      "Episode 5992 finished after 68 timesteps with reward -183\n",
      "Episode 5996 finished after 82 timesteps with reward -278\n",
      "Episode 6157 finished after 82 timesteps with reward -233\n",
      "Episode 6183 finished after 54 timesteps with reward -214\n",
      "Episode 6199 finished after 61 timesteps with reward -230\n",
      "Episode 6288 finished after 88 timesteps with reward -293\n",
      "Episode 6366 finished after 26 timesteps with reward -51\n",
      "Episode 6380 finished after 63 timesteps with reward -205\n",
      "Episode 6663 finished after 56 timesteps with reward -162\n",
      "Episode 6774 finished after 35 timesteps with reward -141\n",
      "Episode 6786 finished after 78 timesteps with reward -283\n",
      "Episode 6957 finished after 40 timesteps with reward -146\n",
      "Episode 6979 finished after 59 timesteps with reward -138\n",
      "Episode 7024 finished after 58 timesteps with reward -209\n",
      "Episode 7073 finished after 65 timesteps with reward -234\n",
      "Episode 7081 finished after 36 timesteps with reward -115\n",
      "Episode 7118 finished after 86 timesteps with reward -309\n",
      "Episode 7284 finished after 56 timesteps with reward -153\n",
      "Episode 7416 finished after 60 timesteps with reward -229\n",
      "Episode 7446 finished after 25 timesteps with reward -68\n",
      "Episode 7539 finished after 68 timesteps with reward -246\n",
      "Episode 7558 finished after 55 timesteps with reward -269\n",
      "Episode 7559 finished after 65 timesteps with reward -207\n",
      "Episode 7576 finished after 81 timesteps with reward -286\n",
      "Episode 7611 finished after 92 timesteps with reward -360\n",
      "Episode 7680 finished after 92 timesteps with reward -315\n",
      "Episode 7770 finished after 87 timesteps with reward -247\n",
      "Episode 7985 finished after 99 timesteps with reward -349\n",
      "Episode 8023 finished after 60 timesteps with reward -148\n",
      "Episode 8084 finished after 57 timesteps with reward -172\n",
      "Episode 8158 finished after 99 timesteps with reward -340\n",
      "Episode 8161 finished after 89 timesteps with reward -339\n",
      "Episode 8176 finished after 92 timesteps with reward -288\n",
      "Episode 8357 finished after 96 timesteps with reward -301\n",
      "Episode 8429 finished after 46 timesteps with reward -152\n",
      "Episode 8454 finished after 97 timesteps with reward -383\n",
      "Episode 8560 finished after 60 timesteps with reward -229\n",
      "Episode 8608 finished after 91 timesteps with reward -233\n",
      "Episode 8631 finished after 86 timesteps with reward -282\n",
      "Episode 8662 finished after 63 timesteps with reward -232\n",
      "Episode 8861 finished after 65 timesteps with reward -198\n",
      "Episode 8889 finished after 79 timesteps with reward -230\n",
      "Episode 8893 finished after 28 timesteps with reward -35\n",
      "Episode 8931 finished after 80 timesteps with reward -375\n",
      "Episode 8946 finished after 55 timesteps with reward -215\n",
      "Episode 8957 finished after 61 timesteps with reward -158\n",
      "Episode 9021 finished after 92 timesteps with reward -324\n",
      "Episode 9089 finished after 28 timesteps with reward -98\n",
      "Episode 9101 finished after 58 timesteps with reward -155\n",
      "Episode 9123 finished after 63 timesteps with reward -223\n",
      "Episode 9231 finished after 89 timesteps with reward -267\n",
      "Episode 9281 finished after 31 timesteps with reward -155\n",
      "Episode 9295 finished after 98 timesteps with reward -357\n",
      "Episode 9339 finished after 46 timesteps with reward -179\n",
      "Episode 9380 finished after 70 timesteps with reward -212\n",
      "Episode 9474 finished after 36 timesteps with reward -115\n",
      "Episode 9487 finished after 88 timesteps with reward -338\n",
      "Episode 9598 finished after 80 timesteps with reward -258\n",
      "Episode 9795 finished after 64 timesteps with reward -233\n",
      "Episode 9999\r"
     ]
    }
   ],
   "source": [
    "for epsilon in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state_index = state[0] \n",
    "    done = False\n",
    "    t_reward = 0\n",
    "\n",
    "    print(f\"Episode {epsilon}\", end=\"\\r\")\n",
    "    for i in range(max_steps):\n",
    "        action = select_action(state_index)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        old_value = q_table[state_index, action]\n",
    "\n",
    "        q_table[state_index, action] = (1 - alpha) * old_value + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "\n",
    "        t_reward += reward\n",
    "        state_index = next_state\n",
    "\n",
    "        if done or truncated:\n",
    "            print(f\"Episode {epsilon} finished after {i} timesteps with reward {t_reward}\")\n",
    "            break\n",
    "\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 15 timesteps with reward 20\n",
      "Episode 1 finished after 15 timesteps with reward 20\n",
      "Episode 2 finished after 9 timesteps with reward 20\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3', render_mode='human')\n",
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    state_index = state[0]\n",
    "    done = False \n",
    "    for i in range(max_steps):\n",
    "        env.render()\n",
    "        action = np.argmax(q_table[state_index])\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        state_index = next_state\n",
    "        if done or truncated:\n",
    "            print(f\"Episode {episode} finished after {i} timesteps with reward {reward}\")\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
